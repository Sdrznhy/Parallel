# 10-CUDA并行矩阵乘法

## 实验目的

CUDA实现并行通用矩阵乘法，并通过实验分析不同线程块大小，访存方式、数据/任务划分方式对并行性能的影响。

## 实验过程和核心代码

### 普通乘法

将要进行的计算记为$A\times B = C$

要实现并行矩阵乘法，可以使每个线程计算$C$中的一个元素，每个线程的工作为：

- 从矩阵 A 中读取一行向量$A[Row * width + i]$
- 从矩阵 B 中读取一列向量$B[i * width + Col]$
- 对这两个向量做点积运算 $A[Row * width + i] * B[i * width + Col]$
- 最后将结果写回矩阵 $C[Row * width + Col] = sum$

核函数如下

```C++
__global__ void matrixMulCUDA(float* A, float* B, float* C, int n)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;

    if (row < n && col < n) {
        for (int k = 0; k < n; ++k) {
            sum += A[row * n + k] * B[k * n + col];
        }
        C[row * n + col] = sum;
    }
}
```

### 优化：共享内存

由于warp的存在，依然会有重复访存的现象

对于矩阵 A 的读取操作，通过合并访问（32 个 thread 访问 Global Memory 的同一地址，合并为一次访问），实际重复读取次数是(n/32)；

对于矩阵 B 的读取操作，通过合并访问（8 个 thread 访问 32 Byte 数据可合并为一次），实际重复读取次数为(m/8)次。

相较于 Global Memory 400~600 个时钟周期的访问延迟，Shared Memory 延时小 20-30 倍、带宽高 10 倍，具有低延时、高带宽的特性。因此性能优化的问题可以转变为如何利用 Shared Memory 代替 Global Memory 来实现数据的重复访问。

让一个 block 内的 thread 先从 Global Memory 中读取子矩阵块数据（大小为 BLOCK_SIZE × BLOCK_SIZE）并写入 Shared Memory 中；在计算时，从 Shared Memory 中读取数据做乘累加，从而避免每次都到 Global 中取数据带来的高延迟影响。接下来让子矩阵块分别在矩阵 A 的行向以及矩阵 B 的列向上**滑动**，直到计算完所有 width 个元素的乘累加。

代码如下：

```C++
__global__ void matmul_ShareMemory(float* M, float* N, float* P, int n)
{
    __shared__ float Mds[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Nds[BLOCK_SIZE][BLOCK_SIZE];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int Col = bx * BLOCK_SIZE + tx;
    int Row = by * BLOCK_SIZE + ty;

    int sum = 0;
    // 循环计算一个块的大小
    for (int i = 0; i < width / BLOCK_SIZE; i++) {
        Mds[ty][tx] = M[Row * n + (i * BLOCK_SIZE + tx)];
        Nds[ty][tx] = N[Col + (i * BLOCK_SIZE + ty) * n];
        __syncthreads();

        // BLOCK_SIZE相乘
        for (int k = 0; k < BLOCK_SIZE; k++)
            sum += Mds[ty][k] * Nds[k][tx];
        __syncthreads();
    }
    P[Row * width + Col] = sum;
}

```

## 实验结果

### 普通方法

#### Block size: (8, 8)

| Matrix size | Time (seconds) | GFLOPS     |
| ----------- | -------------- | ---------- |
| 128         | 0.000258       | 16.256992  |
| 256         | 0.000335       | 100.162484 |
| 512         | 0.000759       | 353.669903 |
| 1024        | 0.004114       | 521.994081 |
| 2048        | 0.030299       | 567.011096 |
| 4096        | 0.428444       | 320.786272 |
| 8192        | 3.361666       | 327.073430 |

#### Block size: (16, 16)

| Matrix size | Time (seconds) | GFLOPS     |
| ----------- | -------------- | ---------- |
| 128         | 0.000310       | 13.530013  |
| 256         | 0.000324       | 103.563062 |
| 512         | 0.000611       | 439.337899 |
| 1024        | 0.002744       | 782.610659 |
| 2048        | 0.024277       | 707.660303 |
| 4096        | 0.189259       | 726.195074 |
| 8192        | 1.383147       | 794.934759 |

#### Block size: (32, 32)

| Matrix size | Time (seconds) | GFLOPS     |
| ----------- | -------------- | ---------- |
| 128         | 0.000227       | 18.477110  |
| 256         | 0.000289       | 116.105301 |
| 512         | 0.000527       | 509.365192 |
| 1024        | 0.002536       | 846.799546 |
| 2048        | 0.017731       | 968.917105 |
| 4096        | 0.190583       | 721.150121 |
| 8192        | 1.371077       | 801.932807 |

### 共享内存

#### Block size: (8, 8)

| Matrix size | Time (seconds) | GFLOPS     |
| ----------- | -------------- | ---------- |
| 128         | 0.000237       | 17.697485  |
| 256         | 0.000315       | 106.522006 |
| 512         | 0.000616       | 435.771844 |
| 1024        | 0.003390       | 633.476002 |
| 2048        | 0.025170       | 682.553404 |
| 4096        | 0.212224       | 647.612680 |
| 8192        | 1.711691       | 642.354039 |

#### Block size: (16, 16)

| Matrix size | Time (seconds) | GFLOPS      |
| ----------- | -------------- | ----------- |
| 128         | 0.000306       | 13.706876   |
| 256         | 0.000281       | 119.410790  |
| 512         | 0.000522       | 514.244169  |
| 1024        | 0.002463       | 871.897543  |
| 2048        | 0.016817       | 1021.577522 |
| 4096        | 0.175083       | 784.993137  |
| 8192        | 1.295473       | 848.733727  |

#### Block size: (32, 32)

| Matrix size | Time (seconds) | GFLOPS     |
| ----------- | -------------- | ---------- |
| 128         | 0.000284       | 14.768676  |
| 256         | 0.000358       | 93.727464  |
| 512         | 0.000593       | 452.673619 |
| 1024        | 0.003213       | 668.373373 |
| 2048        | 0.022645       | 758.660595 |
| 4096        | 0.227816       | 603.289293 |
| 8192        | 1.719341       | 639.495963 |

### 结果分析

GPU：RTX 4060 laptop

1. **共享内存：**

   - 提高数据重用率：共享内存可以减少对全局内存的访问，提高数据重用率，从而提升计算性能。
   - 减少延迟：由于共享内存的访问速度比全局内存快很多，使用共享内存可以减少数据传输延迟，提升整体性能。
   - 在当前设备并没有充分体现出理论优势，可能与矩阵规模和硬件规格有关

2. **块大小的影响：**

   - (8, 8)块大小：适合小矩阵，GFLOPS增幅显著，但在大矩阵时性能提升趋于平缓。
   - (16, 16)块大小：在中等大小和大矩阵时表现最好，GFLOPS值最高。
   - (32, 32)块大小：对于大矩阵，使用共享内存的效果更明显，但不使用共享内存时也有较高的GFLOPS。

   块大小为16在CUDA矩阵乘法中总体优于块大小为32的主要原因可能是它更好地平衡了共享内存的使用、寄存器的分配、线程块的调度以及计算和内存访问的效率。这种平衡使得块大小为16的线程块在大多数情况下能够提供更高的性能和更稳定的GFLOPS表现。

3. **矩阵大小的影响：**

   - 随着矩阵大小的增加，使用共享内存的优势更加明显，特别是在处理大规模矩阵时，共享内存的高效性体现得更为突出。

## 实验感想

对CUDA的使用更加熟悉了，了解了若干种CUDA程序的性能分析方法和优化方式

